{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Fazil Raja\n",
    "## Student Email: fazilraja11@ou.edu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | \n",
    "| -- | -- | -- | -- | -- | \n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files (Required)\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ef0afb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VA Norfolk.pdf\n",
      "KY Louisville.pdf\n",
      "MN Minneapolis St Paul.pdf\n",
      "CA Oceanside.pdf\n",
      "DC_0.pdf\n",
      "CA Chula Vista.pdf\n",
      "FL Jacksonville.pdf\n",
      "TN Memphis.pdf\n",
      "IA Des Moines.pdf\n",
      "OH Toledo.pdf\n",
      "NJ Newark.pdf\n",
      "NC Charlotte.pdf\n",
      "NC Raleigh.pdf\n",
      "LA New Orleans.pdf\n",
      "CA Moreno Valley.pdf\n",
      "MO St. Louis.pdf\n",
      "IN Indianapolis.pdf\n",
      "AL Montgomery.pdf\n",
      "NC Greensboro.pdf\n",
      "FL St. Petersburg.pdf\n",
      "CA Riverside.pdf\n",
      "NE Omaha.pdf\n",
      "TN Chattanooga.pdf\n",
      "NY Albany Troy Schenectady Saratoga Springs.pdf\n",
      "CT NewHaven.pdf\n",
      "LA Baton Rouge.pdf\n",
      "RI Providence.pdf\n",
      "OH Akron.pdf\n",
      "OK Oklahoma City.pdf\n",
      "FL Orlando.pdf\n",
      "MD Baltimore.pdf\n",
      "VA Virginia Beach.pdf\n",
      "VA Richmond.pdf\n",
      "CA Oakland.pdf\n",
      "FL Tampa.pdf\n",
      "WA Spokane.pdf\n",
      "MA Boston.pdf\n",
      "CA Long Beach.pdf\n",
      "AZ Scottsdale AZ.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(532, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(535, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(538, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(541, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(544, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(547, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(550, 0, 140693742553120)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(553, 0, 140693742553120)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN Nashville.pdf\n",
      "NY Mt Vernon Yonkers New Rochelle.pdf\n",
      "NE Lincoln.pdf\n",
      "OH Canton.pdf\n",
      "SC Greenville.pdf\n",
      "FL Miami.pdf\n",
      "CA Sacramento.pdf\n",
      "AZ Tucson.pdf\n",
      "GA Brookhaven.pdf\n",
      "NY Buffalo.pdf\n",
      "LA Shreveport.pdf\n",
      "TX Lubbock.pdf\n",
      "NY Rochester.pdf\n",
      "CA Fremont.pdf\n",
      "NV Las Vegas.pdf\n",
      "MI Port Huron and Marysville.pdf\n",
      "AL Birmingham.pdf\n",
      "GA Atlanta.pdf\n",
      "WI Madison.pdf\n",
      "VA Newport News.pdf\n",
      "WA Seattle.pdf\n",
      "OK Tulsa.pdf\n",
      "NJ Jersey City.pdf\n",
      "OH Cleveland.pdf\n",
      "MI Detroit.pdf\n",
      "CA San Jose_0.pdf\n",
      "NV Reno.pdf\n",
      "CA Fresno.pdf\n",
      "AK Anchorage.pdf\n",
      "FL Tallahassee.pdf\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "\n",
    "directory = 'smartcity'\n",
    "files_data = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'rb') as pdfFileObj:\n",
    "            pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "            num_pages = len(pdfReader.pages)\n",
    "            text = \"\"\n",
    "            for i in range(num_pages):\n",
    "                page = pdfReader.pages[i]\n",
    "                page_text = page.extract_text()\n",
    "                page_lines = page_text.splitlines()\n",
    "                text += page_text\n",
    "            files_data[filename] = text\n",
    "            print(filename)\n",
    "            continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e4905f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added it above\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {},
   "source": [
    "## Cleaning Up PDFs (Required)\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8142e498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Fazil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['parser'])\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_cities_states(text) :\n",
    "    labels = ['GPE', 'LOC', 'PERSON', 'ORG']\n",
    "\n",
    "    cities_states = ['Brookhaven', 'Tallahassee', 'Buffalo', 'Riverside', 'Scottsdale', 'Jacksonville', 'New Orleans', 'Montgomery', 'Port Huron', 'Marysville', 'Seattle', 'Shreveport', 'Spokane', 'Indianapolis', 'Birmingham', 'Baton Rouge', 'Miami', 'Oceanside', 'San Jose', 'Lincoln', 'Boston', 'Sacramento', 'Richmond', 'Atlanta', 'Rochester', 'Memphis', 'Raleigh', 'Albany', 'Troy', 'Schenectady', 'Saratoga Springs', 'Cleveland', 'Charlotte', 'Jersey City', 'Chula Vista', 'Long Beach', 'Detroit', 'Des Moines', 'St. Louis', 'Omaha', 'Akron', 'Newport News', 'Mt Vernon', 'Yonkers', 'New Rochelle', 'Fremont', 'Baltimore', 'Greenville', 'NewHaven', 'Lubbock', 'Fresno', 'Oakland', 'Chattanooga', 'Providence', 'Anchorage', 'Tucson', 'Minneapolis', 'Reno', 'Toledo', 'Greensboro', 'Canton', 'Las Vegas', 'Nashville', 'Oklahoma City', 'Madison', 'Newark', 'Louisville', 'St. Petersburg', 'Moreno Valley', 'Tampa', 'Norfolk', 'Washington, DC', 'Orlando', 'Virginia Beach', 'Tulsa']\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in labels or ent.text in cities_states:\n",
    "            text = text.replace(ent.text, '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus,html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, stopwords=stopword_list, cites_states = True):\n",
    "    \n",
    "    normalized_corpus = {}\n",
    "    # normalize each document in the corpus\n",
    "    count = 0\n",
    "    for key in corpus.keys():\n",
    "        count += 1\n",
    "        print(count)\n",
    "        doc = corpus[key]\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        \n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove states and cities\n",
    "        if cites_states:\n",
    "            doc = remove_cities_states(doc)\n",
    "            \n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits  \n",
    "        print(\"remove_special_characters\")  \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus[key] = doc\n",
    "    print(\"---------------------returning---------------------\")\n",
    "    return normalized_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {},
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3737fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "remove_special_characters\n",
      "2\n",
      "remove_special_characters\n",
      "3\n",
      "remove_special_characters\n",
      "4\n",
      "remove_special_characters\n",
      "5\n",
      "remove_special_characters\n",
      "6\n",
      "remove_special_characters\n",
      "7\n",
      "remove_special_characters\n",
      "8\n",
      "remove_special_characters\n",
      "9\n",
      "remove_special_characters\n",
      "10\n",
      "remove_special_characters\n",
      "11\n",
      "remove_special_characters\n",
      "12\n",
      "remove_special_characters\n",
      "13\n",
      "remove_special_characters\n",
      "14\n",
      "remove_special_characters\n",
      "15\n",
      "remove_special_characters\n",
      "16\n",
      "remove_special_characters\n",
      "17\n",
      "remove_special_characters\n",
      "18\n",
      "remove_special_characters\n",
      "19\n",
      "remove_special_characters\n",
      "20\n",
      "remove_special_characters\n",
      "21\n",
      "remove_special_characters\n",
      "22\n",
      "remove_special_characters\n",
      "23\n",
      "remove_special_characters\n",
      "24\n",
      "remove_special_characters\n",
      "25\n",
      "remove_special_characters\n",
      "26\n",
      "remove_special_characters\n",
      "27\n",
      "remove_special_characters\n",
      "28\n",
      "remove_special_characters\n",
      "29\n",
      "remove_special_characters\n",
      "30\n",
      "remove_special_characters\n",
      "31\n",
      "remove_special_characters\n",
      "32\n",
      "remove_special_characters\n",
      "33\n",
      "remove_special_characters\n",
      "34\n",
      "remove_special_characters\n",
      "35\n",
      "remove_special_characters\n",
      "36\n",
      "remove_special_characters\n",
      "37\n",
      "remove_special_characters\n",
      "38\n",
      "remove_special_characters\n",
      "39\n",
      "remove_special_characters\n",
      "40\n",
      "remove_special_characters\n",
      "41\n",
      "remove_special_characters\n",
      "42\n",
      "remove_special_characters\n",
      "43\n",
      "remove_special_characters\n",
      "44\n",
      "remove_special_characters\n",
      "45\n",
      "remove_special_characters\n",
      "46\n",
      "remove_special_characters\n",
      "47\n",
      "remove_special_characters\n",
      "48\n",
      "remove_special_characters\n",
      "49\n",
      "remove_special_characters\n",
      "50\n",
      "remove_special_characters\n",
      "51\n",
      "remove_special_characters\n",
      "52\n",
      "remove_special_characters\n",
      "53\n",
      "remove_special_characters\n",
      "54\n",
      "remove_special_characters\n",
      "55\n",
      "remove_special_characters\n",
      "56\n",
      "remove_special_characters\n",
      "57\n",
      "remove_special_characters\n",
      "58\n",
      "remove_special_characters\n",
      "59\n",
      "remove_special_characters\n",
      "60\n",
      "remove_special_characters\n",
      "61\n",
      "remove_special_characters\n",
      "62\n",
      "remove_special_characters\n",
      "63\n",
      "remove_special_characters\n",
      "64\n",
      "remove_special_characters\n",
      "65\n",
      "remove_special_characters\n",
      "66\n",
      "remove_special_characters\n",
      "67\n",
      "remove_special_characters\n",
      "68\n",
      "remove_special_characters\n",
      "69\n",
      "remove_special_characters\n",
      "---------------------returning---------------------\n"
     ]
    }
   ],
   "source": [
    "files_normalized_data = normalize_corpus(files_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {},
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "As of right now I did not remove an applicants as the data looks clean. I did not see any issues with the documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "I used labels to remove Cities and States from the text. I created a list of all the cities used in smartcity/ and used that to remove those names from the text too."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "None for now, as the normalization removed quite a few words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models (Required)\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [0.0831, 2.8213, 2.1941] | [0.0407, 2.2966, 1.5409] | [0.0847, 2.2808, 1.0074] |  |\n",
    "|Hierarchical |[0.1024, 3.1921, 2.0224] | [0.0964, 2.7341, 1.4481] | [0.093, 2.3776, 1.0526] | [S,CH,DB]|\n",
    "|DBSCAN | [0.1337, 1.9308, 2.8541] | [0.1337, 1.9308, 2.8541] | [0.1309, 1.9897, 3.1638] | [S,CH,DB] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|36|\n",
    "|Hierarchical |--|--|--|36|\n",
    "|DBSCAN | X | X | X | 36 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fazil/opt/anaconda3/envs/ta/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0831, 2.8213, 2.1941]\n",
      "[0.1337, 1.9308, 2.8541]\n",
      "[0.1024, 3.1921, 2.0224]\n",
      "k = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fazil/opt/anaconda3/envs/ta/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0407, 2.2966, 1.5409]\n",
      "[0.1337, 1.9308, 2.8541]\n",
      "[0.0964, 2.7341, 1.4481]\n",
      "k = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fazil/opt/anaconda3/envs/ta/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0847, 2.2808, 1.0074]\n",
      "[0.1309, 1.9897, 3.1638]\n",
      "[0.093, 2.3776, 1.0526]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# create tfidf matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(files_normalized_data.values())\n",
    "\n",
    "for k in [9, 18, 36]:\n",
    "    print(\"k = \" + str(k))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(tfidf_matrix)\n",
    "\n",
    "    # dbscan\n",
    "    dbscan = DBSCAN(eps=1, min_samples=k).fit_predict(tfidf_matrix)\n",
    "\n",
    "    # agglomerative\n",
    "    sparseArray = tfidf_matrix.toarray()\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=k).fit_predict(sparseArray)\n",
    "\n",
    "    kmeans_score = silhouette_score(tfidf_matrix, kmeans.labels_)\n",
    "    dbscan_score = silhouette_score(tfidf_matrix, dbscan)\n",
    "    agglomerative_score = silhouette_score(tfidf_matrix, agglomerative)\n",
    "\n",
    "    k_means_davies_bouldin_score = davies_bouldin_score(tfidf_matrix.toarray(), kmeans.labels_)\n",
    "    dbscan_davies_bouldin_score = davies_bouldin_score(tfidf_matrix.toarray(), dbscan)\n",
    "    agglomerative_davies_bouldin_score = davies_bouldin_score(tfidf_matrix.toarray(), agglomerative)\n",
    "\n",
    "    k_means_calinski_harabasz_score = calinski_harabasz_score(tfidf_matrix.toarray(), kmeans.labels_)\n",
    "    dbscan_calinski_harabasz_score = calinski_harabasz_score(tfidf_matrix.toarray(), dbscan)\n",
    "    agglomerative_calinski_harabasz_score = calinski_harabasz_score(tfidf_matrix.toarray(), agglomerative)\n",
    "\n",
    "\n",
    "    print([round(kmeans_score, 4), round(k_means_calinski_harabasz_score, 4), round(k_means_davies_bouldin_score, 4)])\n",
    "    print([round(dbscan_score, 4), round(dbscan_calinski_harabasz_score, 4), round(dbscan_davies_bouldin_score, 4)])\n",
    "    print([round(agglomerative_score, 4), round(agglomerative_calinski_harabasz_score, 4), round(agglomerative_davies_bouldin_score, 4)])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "In order to pick the best k, we need to maximize the silhouette and Calinski score while minimizing the Davies-Bouldin score. I used a for loop to go through each k and print out the values and then compared them. After comparing the values, I picked the k that had the best scores, which was 36."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "None of them as neither of the 3 topics are the best at extracting themes in a model. A better model would be a LDA model which is proven to be best at extracting themes in a model. But if I had a choice I would use K means as it is simple and pretty efficient on large datasets such as ours. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fazil/opt/anaconda3/envs/ta/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VA Norfolk.pdf': 10, 'KY Louisville.pdf': 10, 'MN Minneapolis St Paul.pdf': 10, 'CA Oceanside.pdf': 30, 'DC_0.pdf': 13, 'CA Chula Vista.pdf': 17, 'FL Jacksonville.pdf': 22, 'TN Memphis.pdf': 33, 'IA Des Moines.pdf': 29, 'OH Toledo.pdf': 1, 'NJ Newark.pdf': 12, 'NC Charlotte.pdf': 10, 'NC Raleigh.pdf': 5, 'LA New Orleans.pdf': 2, 'CA Moreno Valley.pdf': 1, 'MO St. Louis.pdf': 4, 'IN Indianapolis.pdf': 10, 'AL Montgomery.pdf': 25, 'NC Greensboro.pdf': 14, 'FL St. Petersburg.pdf': 20, 'CA Riverside.pdf': 10, 'NE Omaha.pdf': 4, 'TN Chattanooga.pdf': 3, 'NY Albany Troy Schenectady Saratoga Springs.pdf': 4, 'CT NewHaven.pdf': 10, 'LA Baton Rouge.pdf': 4, 'RI Providence.pdf': 33, 'OH Akron.pdf': 18, 'OK Oklahoma City.pdf': 34, 'FL Orlando.pdf': 10, 'MD Baltimore.pdf': 6, 'VA Virginia Beach.pdf': 7, 'VA Richmond.pdf': 11, 'CA Oakland.pdf': 2, 'FL Tampa.pdf': 15, 'WA Spokane.pdf': 31, 'MA Boston.pdf': 2, 'CA Long Beach.pdf': 10, 'AZ Scottsdale AZ.pdf': 21, 'TN Nashville.pdf': 10, 'NY Mt Vernon Yonkers New Rochelle.pdf': 16, 'NE Lincoln.pdf': 4, 'OH Canton.pdf': 9, 'SC Greenville.pdf': 23, 'FL Miami.pdf': 32, 'CA Sacramento.pdf': 26, 'AZ Tucson.pdf': 10, 'GA Brookhaven.pdf': 8, 'NY Buffalo.pdf': 10, 'LA Shreveport.pdf': 3, 'TX Lubbock.pdf': 1, 'NY Rochester.pdf': 0, 'CA Fremont.pdf': 28, 'NV Las Vegas.pdf': 10, 'MI Port Huron and Marysville.pdf': 27, 'AL Birmingham.pdf': 24, 'GA Atlanta.pdf': 19, 'WI Madison.pdf': 10, 'VA Newport News.pdf': 35, 'WA Seattle.pdf': 26, 'OK Tulsa.pdf': 10, 'NJ Jersey City.pdf': 2, 'OH Cleveland.pdf': 7, 'MI Detroit.pdf': 10, 'CA San Jose_0.pdf': 10, 'NV Reno.pdf': 1, 'CA Fresno.pdf': 10, 'AK Anchorage.pdf': 10, 'FL Tallahassee.pdf': 1}\n"
     ]
    }
   ],
   "source": [
    "text_data = list(files_normalized_data.values())\n",
    "text_vectors = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "k  = 36\n",
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(text_vectors)\n",
    "\n",
    "cluster_ids = kmeans.predict(text_vectors)\n",
    "files_cluster_data = files_normalized_data.copy()\n",
    "\n",
    "for i, city in enumerate(files_normalized_data.keys()):\n",
    "    files_cluster_data[city] = cluster_ids[i]\n",
    "\n",
    "print(files_cluster_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 20353)\n",
      "<class 'list'>\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(kmeans, 'model.pkl')\n",
    "\n",
    "pred = kmeans.predict(text_vectors[0])\n",
    "print(kmeans.cluster_centers_.shape)\n",
    "print(type(text_data))\n",
    "print(type(text_vectors[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts (Required)\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>information, improve, make, region, corridor, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>hr, oakland, awalk, vaa, seeclickfix, difficul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>challengeapplication, traic, synertic, eective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>sign, chlenge, page, fiber, emission, projt, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>opportunity, mile, planning, innovation, desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>oks, dash, wegolook, oklahoma, frail, itn, ini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>transporta, innova, eff, exis, addi, iden, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>hy, vee, transload, plandsm, legally, plugless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>commication, commity, opportitie, fding, arod,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>ewark, mobity, anchorage, devery, utize, ew, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>datum, city, system, vehicle, transportation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>buford, marta, english, opose, ssell, cotton, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>es, pment, te, ed, propos, tem, ion, ment, co,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>verty, dision, rhood, treet, oprtunity, transr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>ffi, cs, ons, onal, na, commi, commity, ini, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic21</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic22</th>\n",
       "      <td>atop, ips, vti, opportity, clutter, tariff, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic23</th>\n",
       "      <td>jax, orth, fixture, rity, metrans, trade, etoa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic24</th>\n",
       "      <td>pinellas, thnologie, connte, connted, pport, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic25</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic26</th>\n",
       "      <td>chlen, nagement, niversity, madisons, intain, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic27</th>\n",
       "      <td>component, dtfhra, count, appcation, end, bike...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic28</th>\n",
       "      <td>simulator, teaching, programmable, pn, radical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic29</th>\n",
       "      <td>sdot, binational, serda, mast, mapa, pop, virt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic30</th>\n",
       "      <td>disclosure, title, parkme, hs, mate, hav, bind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic31</th>\n",
       "      <td>beyod, challege, traic, mont, sem, winston, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic32</th>\n",
       "      <td>pky, river, uva, bronx, mill, hill, beeline, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic33</th>\n",
       "      <td>aa, php, photovoltaic, photography, photograph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic34</th>\n",
       "      <td>sity, stth, roach, sacrt, clearinghouse, golde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic35</th>\n",
       "      <td>also, vision, would, downtown, local, propose,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic36</th>\n",
       "      <td>roject, ferry, shall, progression, synergistic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Terms per Topic\n",
       "Topic1   information, improve, make, region, corridor, ...\n",
       "Topic2   aa, php, photovoltaic, photography, photograph...\n",
       "Topic3   hr, oakland, awalk, vaa, seeclickfix, difficul...\n",
       "Topic4   challengeapplication, traic, synertic, eective...\n",
       "Topic5   sign, chlenge, page, fiber, emission, projt, l...\n",
       "Topic6   opportunity, mile, planning, innovation, desig...\n",
       "Topic7   oks, dash, wegolook, oklahoma, frail, itn, ini...\n",
       "Topic8   transporta, innova, eff, exis, addi, iden, cri...\n",
       "Topic9   aa, php, photovoltaic, photography, photograph...\n",
       "Topic10  aa, php, photovoltaic, photography, photograph...\n",
       "Topic11  aa, php, photovoltaic, photography, photograph...\n",
       "Topic12  hy, vee, transload, plandsm, legally, plugless...\n",
       "Topic13  commication, commity, opportitie, fding, arod,...\n",
       "Topic14  aa, php, photovoltaic, photography, photograph...\n",
       "Topic15  ewark, mobity, anchorage, devery, utize, ew, j...\n",
       "Topic16  datum, city, system, vehicle, transportation, ...\n",
       "Topic17  buford, marta, english, opose, ssell, cotton, ...\n",
       "Topic18  es, pment, te, ed, propos, tem, ion, ment, co,...\n",
       "Topic19  verty, dision, rhood, treet, oprtunity, transr...\n",
       "Topic20  ffi, cs, ons, onal, na, commi, commity, ini, f...\n",
       "Topic21  aa, php, photovoltaic, photography, photograph...\n",
       "Topic22  atop, ips, vti, opportity, clutter, tariff, co...\n",
       "Topic23  jax, orth, fixture, rity, metrans, trade, etoa...\n",
       "Topic24  pinellas, thnologie, connte, connted, pport, k...\n",
       "Topic25  aa, php, photovoltaic, photography, photograph...\n",
       "Topic26  chlen, nagement, niversity, madisons, intain, ...\n",
       "Topic27  component, dtfhra, count, appcation, end, bike...\n",
       "Topic28  simulator, teaching, programmable, pn, radical...\n",
       "Topic29  sdot, binational, serda, mast, mapa, pop, virt...\n",
       "Topic30  disclosure, title, parkme, hs, mate, hav, bind...\n",
       "Topic31  beyod, challege, traic, mont, sem, winston, ac...\n",
       "Topic32  pky, river, uva, bronx, mill, hill, beeline, s...\n",
       "Topic33  aa, php, photovoltaic, photography, photograph...\n",
       "Topic34  sity, stth, roach, sacrt, clearinghouse, golde...\n",
       "Topic35  also, vision, would, downtown, local, propose,...\n",
       "Topic36  roject, ferry, shall, progression, synergistic..."
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "TOTAL_TOPICS = 36\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=TOTAL_TOPICS, random_state=0)\n",
    "\n",
    "doc = lda_model.fit(text_vectors)\n",
    "\n",
    "topic_word = lda_model.components_\n",
    "\n",
    "vocab = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "topic_key_terms_idxs = np.argsort(-np.absolute(topic_word), axis=1)[:, :36]\n",
    "topic_keyterms = vocab[topic_key_terms_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "topics_df = pd.DataFrame(topics, columns = ['Terms per Topic'], index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "Topic1: Regional Development and Corridor Improvement\n",
    "Topic2: Photography and Photovoltaic Technology\n",
    "Topic3: Human Resources and Civic Engagement in Oakland\n",
    "Topic4: Effective Traffic Management and Synergetic Applications\n",
    "Topic5: Signage and Fiber Emission Reduction Projects\n",
    "Topic6: Urban Planning and Innovative Design Opportunities\n",
    "Topic7: Frailty and Mobility in Oklahoma: Initiatives and Innovations\n",
    "Topic8: Efficient and Innovative Transportation Solutions\n",
    "Topic9: Photography and Photovoltaic Technology\n",
    "Topic10: Photography and Photovoltaic Technology\n",
    "Topic11: Photography and Photovoltaic Technology\n",
    "Topic12: Legally-compliant Transloading and Plugless Technology\n",
    "Topic13: Community Communication and Funding Opportunities\n",
    "Topic14: Photography and Photovoltaic Technology\n",
    "Topic15: Mobility Solutions for Newark and Anchorage\n",
    "Topic16: Data-driven Transportation Systems and Vehicles\n",
    "Topic17: Cotton Belt Rail Line and MARTA in English Avenue\n",
    "Topic18: Development and Implementation of Educational Programs\n",
    "Topic19: Poverty Reduction, Neighborhood Revitalization, and Transportation Access\n",
    "Topic20: Federal Initiatives and Community-based Programs\n",
    "Topic21: Photography and Photovoltaic Technology\n",
    "Topic22: Clutter Reduction and Tariff Optimization in Transit\n",
    "Topic23: Trade, Security, and Infrastructure in Jacksonville\n",
    "Topic24: Pinellas County Technological Advancements and Connectivity\n",
    "Topic25: Photography and Photovoltaic Technology\n",
    "Topic26: University Management and Sustainable Maintenance\n",
    "Topic27: Bike Counts and App-based Transportation Solutions\n",
    "Topic28: Programmable Simulators for Radical Teaching\n",
    "Topic29: SDOT and Bi-National Mapping and Planning\n",
    "Topic30: Title Disclosure and Parking Management\n",
    "Topic31: Traffic Management Beyond Winston-Salem Challenges\n",
    "Topic32: River Parkway Development and Beeline Bus Service\n",
    "Topic33: Photography and Photovoltaic Technology\n",
    "Topic34: Regional Approaches to Transportation in St. Thomas\n",
    "Topic35: Local Vision and Downtown Proposals\n",
    "Topic36: Ferry Projects and Synergistic Progression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e937841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: VA Norfolk.pdf\n",
      "Topic (15, 0.458828416126366): (0, 0.14817431876993228)\n",
      "\n",
      "\n",
      "Document 1: KY Louisville.pdf\n",
      "Topic (15, 0.41276745669931014): (34, 0.33353207488892866)\n",
      "\n",
      "\n",
      "Document 2: MN Minneapolis St Paul.pdf\n",
      "Topic (15, 0.4694562157330299): (34, 0.12673912759703665)\n",
      "\n",
      "\n",
      "Document 3: CA Oceanside.pdf\n",
      "Topic (15, 0.4193131265597532): (21, 0.11341321129327087)\n",
      "\n",
      "\n",
      "Document 4: DC_0.pdf\n",
      "Topic (15, 0.409184239876521): (0, 0.2446824059487731)\n",
      "\n",
      "\n",
      "Document 5: CA Chula Vista.pdf\n",
      "Topic (17, 0.644166154033635): (15, 0.1341613656965425)\n",
      "\n",
      "\n",
      "Document 6: FL Jacksonville.pdf\n",
      "Topic (15, 0.37311080656869655): (0, 0.1265661346024401)\n",
      "\n",
      "\n",
      "Document 7: TN Memphis.pdf\n",
      "Topic (15, 0.4764023948034659): (0, 0.1251536293332473)\n",
      "\n",
      "\n",
      "Document 8: IA Des Moines.pdf\n",
      "Topic (15, 0.46445217871114564): (0, 0.16227348316384468)\n",
      "\n",
      "\n",
      "Document 9: OH Toledo.pdf\n",
      "Topic (0, 0.027777777777777776): (1, 0.027777777777777776)\n",
      "\n",
      "\n",
      "Document 10: NJ Newark.pdf\n",
      "Topic (15, 0.29191162579949353): (14, 0.20391791487669184)\n",
      "\n",
      "\n",
      "Document 11: NC Charlotte.pdf\n",
      "Topic (15, 0.4415892274786327): (5, 0.27610611265128837)\n",
      "\n",
      "\n",
      "Document 12: NC Raleigh.pdf\n",
      "Topic (15, 0.41839239261302663): (0, 0.15438492210353277)\n",
      "\n",
      "\n",
      "Document 13: LA New Orleans.pdf\n",
      "Topic (15, 0.4570434289366555): (34, 0.12757110360089258)\n",
      "\n",
      "\n",
      "Document 14: CA Moreno Valley.pdf\n",
      "Topic (0, 0.027777777777777776): (1, 0.027777777777777776)\n",
      "\n",
      "\n",
      "Document 15: MO St. Louis.pdf\n",
      "Topic (15, 0.44315963930885716): (4, 0.19510973951682595)\n",
      "\n",
      "\n",
      "Document 16: IN Indianapolis.pdf\n",
      "Topic (15, 0.5139774807842649): (0, 0.15360269651687336)\n",
      "\n",
      "\n",
      "Document 17: AL Montgomery.pdf\n",
      "Topic (5, 0.40609999901233107): (15, 0.36709505893321387)\n",
      "\n",
      "\n",
      "Document 18: NC Greensboro.pdf\n",
      "Topic (15, 0.446452661457062): (0, 0.14225243946629343)\n",
      "\n",
      "\n",
      "Document 19: FL St. Petersburg.pdf\n",
      "Topic (15, 0.42348504145817656): (34, 0.13998582129198903)\n",
      "\n",
      "\n",
      "Document 20: CA Riverside.pdf\n",
      "Topic (15, 0.5065821003105386): (34, 0.13451254146070177)\n",
      "\n",
      "\n",
      "Document 21: NE Omaha.pdf\n",
      "Topic (15, 0.4817086209686545): (0, 0.1498945377265191)\n",
      "\n",
      "\n",
      "Document 22: TN Chattanooga.pdf\n",
      "Topic (15, 0.42080912577433643): (26, 0.15232031186966585)\n",
      "\n",
      "\n",
      "Document 23: NY Albany Troy Schenectady Saratoga Springs.pdf\n",
      "Topic (0, 0.3809160783874433): (15, 0.3801582288227566)\n",
      "\n",
      "\n",
      "Document 24: CT NewHaven.pdf\n",
      "Topic (15, 0.43956448362693296): (0, 0.13794728809712942)\n",
      "\n",
      "\n",
      "Document 25: LA Baton Rouge.pdf\n",
      "Topic (15, 0.43111383062767966): (0, 0.1391781281175327)\n",
      "\n",
      "\n",
      "Document 26: RI Providence.pdf\n",
      "Topic (15, 0.4794306178096615): (34, 0.2037826969602169)\n",
      "\n",
      "\n",
      "Document 27: OH Akron.pdf\n",
      "Topic (23, 0.4138927879849322): (15, 0.25065853981425534)\n",
      "\n",
      "\n",
      "Document 28: OK Oklahoma City.pdf\n",
      "Topic (15, 0.4038708178151151): (6, 0.14474822069962645)\n",
      "\n",
      "\n",
      "Document 29: FL Orlando.pdf\n",
      "Topic (15, 0.42409043483795245): (34, 0.3017008230044617)\n",
      "\n",
      "\n",
      "Document 30: MD Baltimore.pdf\n",
      "Topic (15, 0.3614162200713195): (25, 0.14056787624203035)\n",
      "\n",
      "\n",
      "Document 31: VA Virginia Beach.pdf\n",
      "Topic (15, 0.3821169749305282): (23, 0.13658311246755986)\n",
      "\n",
      "\n",
      "Document 32: VA Richmond.pdf\n",
      "Topic (15, 0.300281444017963): (0, 0.16852540987174597)\n",
      "\n",
      "\n",
      "Document 33: CA Oakland.pdf\n",
      "Topic (15, 0.4807943442689798): (0, 0.14279297834577953)\n",
      "\n",
      "\n",
      "Document 34: FL Tampa.pdf\n",
      "Topic (15, 0.38418996925055815): (0, 0.35613581588868387)\n",
      "\n",
      "\n",
      "Document 35: WA Spokane.pdf\n",
      "Topic (4, 0.3830514384136652): (15, 0.33226269002206493)\n",
      "\n",
      "\n",
      "Document 36: MA Boston.pdf\n",
      "Topic (15, 0.4323801287199199): (0, 0.13712598325285652)\n",
      "\n",
      "\n",
      "Document 37: CA Long Beach.pdf\n",
      "Topic (15, 0.4584284666588474): (0, 0.13405167275955454)\n",
      "\n",
      "\n",
      "Document 38: AZ Scottsdale AZ.pdf\n",
      "Topic (15, 0.7315373805479061): (0, 0.05566303483039896)\n",
      "\n",
      "\n",
      "Document 39: TN Nashville.pdf\n",
      "Topic (15, 0.4168279395798304): (0, 0.128100035309003)\n",
      "\n",
      "\n",
      "Document 40: NY Mt Vernon Yonkers New Rochelle.pdf\n",
      "Topic (31, 0.7096017961421649): (15, 0.05127887421203679)\n",
      "\n",
      "\n",
      "Document 41: NE Lincoln.pdf\n",
      "Topic (15, 0.41355660005122874): (26, 0.22305394053568095)\n",
      "\n",
      "\n",
      "Document 42: OH Canton.pdf\n",
      "Topic (15, 0.3583292343058054): (17, 0.14346283054059658)\n",
      "\n",
      "\n",
      "Document 43: SC Greenville.pdf\n",
      "Topic (15, 0.40006303465989285): (14, 0.1615345441138263)\n",
      "\n",
      "\n",
      "Document 44: FL Miami.pdf\n",
      "Topic (15, 0.4611800507483067): (34, 0.13898849611715583)\n",
      "\n",
      "\n",
      "Document 45: CA Sacramento.pdf\n",
      "Topic (15, 0.48346869823011224): (34, 0.13142100556912628)\n",
      "\n",
      "\n",
      "Document 46: AZ Tucson.pdf\n",
      "Topic (15, 0.732186943520647): (0, 0.07340328764448353)\n",
      "\n",
      "\n",
      "Document 47: GA Brookhaven.pdf\n",
      "Topic (15, 0.3265181095958948): (0, 0.13740880383625573)\n",
      "\n",
      "\n",
      "Document 48: NY Buffalo.pdf\n",
      "Topic (15, 0.4926026978825593): (0, 0.14795045055242156)\n",
      "\n",
      "\n",
      "Document 49: LA Shreveport.pdf\n",
      "Topic (15, 0.35685092803870394): (34, 0.3428437098836834)\n",
      "\n",
      "\n",
      "Document 50: TX Lubbock.pdf\n",
      "Topic (0, 0.027777777777777776): (1, 0.027777777777777776)\n",
      "\n",
      "\n",
      "Document 51: NY Rochester.pdf\n",
      "Topic (15, 0.2794653116022862): (19, 0.27661912314304654)\n",
      "\n",
      "\n",
      "Document 52: CA Fremont.pdf\n",
      "Topic (15, 0.4060282614698878): (5, 0.2849334667251889)\n",
      "\n",
      "\n",
      "Document 53: NV Las Vegas.pdf\n",
      "Topic (15, 0.43751675540894086): (34, 0.12111343698953143)\n",
      "\n",
      "\n",
      "Document 54: MI Port Huron and Marysville.pdf\n",
      "Topic (15, 0.41230284200353173): (0, 0.13351983315178825)\n",
      "\n",
      "\n",
      "Document 55: AL Birmingham.pdf\n",
      "Topic (15, 0.29328868276327247): (7, 0.21749773408446174)\n",
      "\n",
      "\n",
      "Document 56: GA Atlanta.pdf\n",
      "Topic (15, 0.3883257068614139): (17, 0.2563589803295493)\n",
      "\n",
      "\n",
      "Document 57: WI Madison.pdf\n",
      "Topic (15, 0.49353748470507947): (34, 0.13755263280386815)\n",
      "\n",
      "\n",
      "Document 58: VA Newport News.pdf\n",
      "Topic (15, 0.4454730563577935): (35, 0.13643010822081575)\n",
      "\n",
      "\n",
      "Document 59: WA Seattle.pdf\n",
      "Topic (15, 0.6750722513931289): (0, 0.09761081144486222)\n",
      "\n",
      "\n",
      "Document 60: OK Tulsa.pdf\n",
      "Topic (0, 0.4272833335477884): (15, 0.33958953160971916)\n",
      "\n",
      "\n",
      "Document 61: NJ Jersey City.pdf\n",
      "Topic (15, 0.4949559460576714): (0, 0.12951611556588538)\n",
      "\n",
      "\n",
      "Document 62: OH Cleveland.pdf\n",
      "Topic (15, 0.36959503364484453): (4, 0.2523788271137302)\n",
      "\n",
      "\n",
      "Document 63: MI Detroit.pdf\n",
      "Topic (15, 0.7018833392418717): (0, 0.08700621246228951)\n",
      "\n",
      "\n",
      "Document 64: CA San Jose_0.pdf\n",
      "Topic (15, 0.469810018811757): (5, 0.17120698725884884)\n",
      "\n",
      "\n",
      "Document 65: NV Reno.pdf\n",
      "Topic (0, 0.027777777777777776): (1, 0.027777777777777776)\n",
      "\n",
      "\n",
      "Document 66: CA Fresno.pdf\n",
      "Topic (15, 0.45392386247120375): (34, 0.13718548859338925)\n",
      "\n",
      "\n",
      "Document 67: AK Anchorage.pdf\n",
      "Topic (15, 0.43660311006371166): (0, 0.13549786275044196)\n",
      "\n",
      "\n",
      "Document 68: FL Tallahassee.pdf\n",
      "Topic (0, 0.027777777777777776): (1, 0.027777777777777776)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_dist = lda_model.transform(text_vectors)\n",
    "\n",
    "# sort topic distribution for each document\n",
    "top_topics = []\n",
    "for i, doc in enumerate(topic_dist):\n",
    "    sorted_topics = sorted(enumerate(doc), key=lambda x: x[1], reverse=True)\n",
    "    top_topics.append(sorted_topics)\n",
    "\n",
    "keys = list(files_normalized_data.keys())\n",
    "# print top topics for each document with probabilities\n",
    "for i, doc in enumerate(text_vectors):\n",
    "    print(f\"Document {i}: {keys[i]}\")\n",
    "    print(f\"Topic {top_topics[i][0]}: {top_topics[i][1]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e568652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VA Norfolk.pdf': (15, 0), 'KY Louisville.pdf': (15, 34), 'MN Minneapolis St Paul.pdf': (15, 34), 'CA Oceanside.pdf': (15, 21), 'DC_0.pdf': (15, 0), 'CA Chula Vista.pdf': (17, 15), 'FL Jacksonville.pdf': (15, 0), 'TN Memphis.pdf': (15, 0), 'IA Des Moines.pdf': (15, 0), 'OH Toledo.pdf': (0, 1), 'NJ Newark.pdf': (15, 14), 'NC Charlotte.pdf': (15, 5), 'NC Raleigh.pdf': (15, 0), 'LA New Orleans.pdf': (15, 34), 'CA Moreno Valley.pdf': (0, 1), 'MO St. Louis.pdf': (15, 4), 'IN Indianapolis.pdf': (15, 0), 'AL Montgomery.pdf': (5, 15), 'NC Greensboro.pdf': (15, 0), 'FL St. Petersburg.pdf': (15, 34), 'CA Riverside.pdf': (15, 34), 'NE Omaha.pdf': (15, 0), 'TN Chattanooga.pdf': (15, 26), 'NY Albany Troy Schenectady Saratoga Springs.pdf': (0, 15), 'CT NewHaven.pdf': (15, 0), 'LA Baton Rouge.pdf': (15, 0), 'RI Providence.pdf': (15, 34), 'OH Akron.pdf': (23, 15), 'OK Oklahoma City.pdf': (15, 6), 'FL Orlando.pdf': (15, 34), 'MD Baltimore.pdf': (15, 25), 'VA Virginia Beach.pdf': (15, 23), 'VA Richmond.pdf': (15, 0), 'CA Oakland.pdf': (15, 0), 'FL Tampa.pdf': (15, 0), 'WA Spokane.pdf': (4, 15), 'MA Boston.pdf': (15, 0), 'CA Long Beach.pdf': (15, 0), 'AZ Scottsdale AZ.pdf': (15, 0), 'TN Nashville.pdf': (15, 0), 'NY Mt Vernon Yonkers New Rochelle.pdf': (31, 15), 'NE Lincoln.pdf': (15, 26), 'OH Canton.pdf': (15, 17), 'SC Greenville.pdf': (15, 14), 'FL Miami.pdf': (15, 34), 'CA Sacramento.pdf': (15, 34), 'AZ Tucson.pdf': (15, 0), 'GA Brookhaven.pdf': (15, 0), 'NY Buffalo.pdf': (15, 0), 'LA Shreveport.pdf': (15, 34), 'TX Lubbock.pdf': (0, 1), 'NY Rochester.pdf': (15, 19), 'CA Fremont.pdf': (15, 5), 'NV Las Vegas.pdf': (15, 34), 'MI Port Huron and Marysville.pdf': (15, 0), 'AL Birmingham.pdf': (15, 7), 'GA Atlanta.pdf': (15, 17), 'WI Madison.pdf': (15, 34), 'VA Newport News.pdf': (15, 35), 'WA Seattle.pdf': (15, 0), 'OK Tulsa.pdf': (0, 15), 'NJ Jersey City.pdf': (15, 0), 'OH Cleveland.pdf': (15, 4), 'MI Detroit.pdf': (15, 0), 'CA San Jose_0.pdf': (15, 5), 'NV Reno.pdf': (0, 1), 'CA Fresno.pdf': (15, 34), 'AK Anchorage.pdf': (15, 0), 'FL Tallahassee.pdf': (0, 1)}\n"
     ]
    }
   ],
   "source": [
    "# add the topic to the dictionary\n",
    "files_topic_data = files_normalized_data.copy()\n",
    "for i, doc in enumerate(text_vectors):\n",
    "    files_topic_data[keys[i]] = top_topics[i][0][0], top_topics[i][1][0]\n",
    "\n",
    "print(files_topic_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords (Extra Credit Section)\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. Gensim is outdated; try a spacy or nltk method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cbe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b545ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data (Required)\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"smartcity_eda.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "    writer.writerow([\"city\", \"raw text\", \"clean text\", \"cluster id\", \"topic id\"])\n",
    "\n",
    "    for city in files_data.keys():\n",
    "        writer.writerow([city, files_data[city], files_normalized_data[city], files_cluster_data[city], files_topic_data[city]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
